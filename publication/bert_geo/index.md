---
title: "Effects of Architecture and Training on Embedding Geometry and Feature Discriminability in BERT"
authors:
- Maksim Podkorytov
- admin
- Jinglun Cai
- Kobra Amirizirtol
- Xiuwen Liu
date: "2020-07-14T00:00:00Z"
doi: ""

# Schedule page publish date (NOT publication's date).
publishDate: "2017-01-01T00:00:00Z"

# Publication type.
# Legend: 0 = Uncategorized; 1 = Conference paper; 2 = Journal article;
# 3 = Preprint / Working Paper; 4 = Report; 5 = Book; 6 = Book section;
# 7 = Thesis; 8 = Patent
publication_types: ["1"]

# Publication name and optional abbreviated publication name.
publication: In *Source Themes Conference*
publication_short: In *STC*

abstract: Natural language processing has improved substantially in the last few years due to the increased computationalpower and availability of text data. Bidirectional Encoder Representations from Transformers (BERT) have further improved theperformance by using an auto-encoding model that incorporateslarger bidirectional contexts. However, the underlying mechanisms of BERT for its effectiveness are not well understood. In this paper we investigate how the BERT architecture and its pre-training protocol affect the geometry of its embeddings and the effectiveness of its features for classification tasks. As an auto-encoding model, during pre-training, it produces representationsthat are context  dependent and at the same time must beable to “reconstruct” the original input sentences. The complex interactions of the two via transformers lead to interesting geometric properties of the embeddings and subsequently affectthe inherent discriminability of the resulting representations. Our experimental results illustrate that the BERT models do not produce “effective” contextualized representations for words and their improved performance may mainly be due to fine-tuningor classifiers that model the dependencies explicitly by encoding syntactic patterns in the training data.

# Summary. An optional shortened abstract.
summary: In this paper we investigate how the BERT architecture and its pre-training protocol affect the geometry of its embeddings and the effectiveness of its features for classification tasks. As an auto-encoding model, during pre-training, it produces representationsthat are context  dependent and at the same time must beable to “reconstruct” the original input sentences. The complex interactions of the two via transformers lead to interesting geometric properties of the embeddings and subsequently affectthe inherent discriminability of the resulting representations. Our experimental results illustrate that the BERT models do not produce “effective” contextualized representations for words.

tags:
- Source Themes
featured: true

links:
url_pdf: https://wcci2020.org/ijcnn-2020-program/






---



